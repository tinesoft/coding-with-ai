# Learning Validation Contract: Module 3

**Module**: AI-Powered Debugging, Refactoring, and Verification  
**Date**: 2025-10-26  
**Purpose**: Define success criteria and validation mechanisms for module completion

## Learning Objectives Contract

### Objective 1: AI-Assisted Debugging Proficiency

**What Learners Will Achieve**:
- Apply systematic debugging workflow with AI as diagnostic assistant
- Use GitHub Copilot to analyze stack traces and suggest fixes
- Identify root causes of bugs rather than applying fixes blindly
- Validate AI debugging suggestions before implementation

**Measurable Success Criteria**:
- Successfully debug 5-7 intentionally buggy code samples in Lab 31
- Explain root cause for each bug identified (not just the fix)
- Demonstrate at least 2 different AI prompting strategies for debugging
- Complete debugging tasks 30% faster than manual-only approach (self-timed)

**Validation Method**:
- Lab 31 completion with all exercises solved
- Written explanation of debugging process for at least 2 bugs
- Knowledge check questions on debugging workflow

---

### Objective 2: Code Refactoring with AI Tools

**What Learners Will Achieve**:
- Identify common refactoring opportunities in legacy code
- Use AI tools to suggest and apply appropriate refactoring patterns
- Evaluate refactoring suggestions for correctness and maintainability
- Apply refactorings safely with backward compatibility

**Measurable Success Criteria**:
- Successfully refactor 4-6 code samples in Lab 32 using different patterns
- Improve code readability metrics (measured by before/after comparison)
- Identify when AI refactoring suggestions should be rejected
- Maintain functionality through all refactorings (no broken tests)

**Validation Method**:
- Lab 32 completion with refactored code submissions
- Comparison of before/after code quality
- Knowledge check questions on refactoring patterns

---

### Objective 3: AI Code Verification Skills

**What Learners Will Achieve**:
- Apply systematic verification checklist to AI-generated code
- Detect hallucinations, security issues, and logic errors in AI code
- Understand human-in-the-loop validation importance
- Make informed decisions about accepting/rejecting AI suggestions

**Measurable Success Criteria**:
- Identify 7 types of issues in AI-generated code samples (Lab 33)
- Correctly categorize issues by severity (critical, major, minor)
- Explain why each identified issue is problematic
- Demonstrate verification workflow on new AI-generated code

**Validation Method**:
- Lab 33 completion with documented issue findings
- Accuracy of issue identification (compare to solution)
- Knowledge check questions on verification techniques

---

## Assessment Contract

### Knowledge Check Component

**Format**: Multiple-choice and scenario-based questions in slide 37

**Topics Covered**:
1. Systematic debugging workflow steps (2 questions)
2. Appropriate use of AI debugging assistance (2 questions)
3. Refactoring pattern identification (2 questions)
4. AI code verification checklist application (2 questions)
5. When to trust vs. question AI suggestions (2 questions)

**Passing Criteria**: 8/10 questions correct (80%)

**Question Alignment**:
- Each question directly references content from slides 31-33
- Scenarios based on lab exercise types
- No questions on content not taught in module

---

### Lab Completion Component

**Required Labs**:
- Lab 31 (AI Debugging): All exercises completed with explanations
- Lab 32 (AI Refactoring): All refactorings applied successfully
- Lab 33 (Code Verification): All issues identified and categorized

**Passing Criteria**:
- All three labs submitted
- Solutions demonstrate understanding (not just copying)
- Code changes are functional and appropriate

**Validation Process**:
1. Learner completes exercises following lab README
2. Compares results with solution README
3. Verifies solution resources/ contains working examples
4. Self-assesses understanding against success criteria

---

## Content Quality Contract

### Slide Content Requirements

**Educational Standards**:
- Each slide clearly states learning objective
- Content progresses from fundamentals to application
- Examples are practical and relatable
- Technical accuracy verified against current AI tool capabilities

**SFEIR Theme Compliance**:
- Proper slide separators (`##--##`)
- Correct class directives (`exercice`, `transition`, etc.)
- Visual assets displayed on dedicated slides
- Consistent formatting throughout

**Validation Checklist**:
- [ ] All slides follow SFEIR Theme syntax
- [ ] Learning objectives explicitly stated
- [ ] Progressive difficulty from slide 30 → 37
- [ ] No implementation details leak into concepts
- [ ] Assessment questions match taught content

---

### Lab Content Requirements

**Instructional Quality**:
- Clear objectives for each exercise
- Step-by-step instructions are actionable
- Success criteria define expected outcomes
- Prerequisites explicitly stated

**Solution Quality**:
- Complete working code in resources/ folders
- Explanations focus on *why* not just *what*
- Solutions demonstrate best practices
- Multiple valid approaches acknowledged

**Validation Checklist**:
- [ ] Lab README has clear objectives and prerequisites
- [ ] Each exercise has success criteria
- [ ] Solution README matches exercise sequence
- [ ] Solution resources/ contains all deliverable files
- [ ] No "Success Criteria Checklist" sections in labs
- [ ] Solutions are complete and runnable

---

## Alignment Verification Contract

### Cross-Component Alignment

**Slides ↔ Labs**:
- Slide 31 concepts → Lab 31 exercises
- Slide 32 concepts → Lab 32 exercises
- Slide 33 concepts → Lab 33 exercises
- Exercise slides (34-36) → Corresponding labs

**Labs ↔ Assessment**:
- Knowledge check questions reference lab scenarios
- Lab completion verifies practical competency
- Assessment criteria match lab learning objectives

**Content ↔ Constitution**:
- Sequential numbering maintained (slides 30-37, labs 31-33)
- Module-scoped lab numbering followed
- Solution deliverable completeness ensured
- No administrative checklists in learner materials

**Validation Method**:
- Systematic review of all cross-references
- Update content simultaneously when changes occur
- Verify no orphaned references after edits

---

## Success Metrics Contract

### Module-Level Metrics

**Target Outcomes** (from spec.md Success Criteria):
- **SC-001**: 30% faster debugging with AI tools (self-reported timing)
- **SC-002**: 80% of AI refactorings accepted and improve maintainability
- **SC-003**: 100% of code changes reviewed before integration (verified in lab 33)
- **SC-004**: Zero critical vulnerabilities in verified code (measured in lab 33)
- **SC-005**: 80% of learners pass knowledge check and complete all labs

**Measurement Approach**:
- Pre/post lab timing comparisons (debugging speed)
- Code quality metrics before/after refactoring
- Issue detection accuracy in verification exercises
- Assessment completion rates and scores

---

## Validation Summary

This contract ensures:
1. **Clear Learning Objectives**: Measurable, achievable, validated
2. **Aligned Assessment**: Tests match taught content
3. **Quality Standards**: Constitutional compliance throughout
4. **Success Criteria**: Quantifiable outcomes defined
5. **Learner Experience**: Progressive, practical, supported by complete solutions

**Contract Status**: ✅ Ready for content creation (Phase 2)
