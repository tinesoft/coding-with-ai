# Assessment Criteria: Tool Categorization Success (SC-001)

**Module**: Module 2 - Modern AI Coding Tools  
**User Story**: US1 - AI Coding Tools Landscape Overview  
**Success Criterion**: SC-001 - 90% of learners can successfully categorize 8 out of 10 AI coding tools into correct categories within 5 minutes

## Assessment Overview

### Objective
Validate learner understanding of AI coding tools landscape through practical categorization exercise.

### Time Limit
5 minutes maximum for 10-tool categorization task.

### Success Threshold
- **Minimum Performance**: 8/10 tools correctly categorized (80% accuracy)
- **Target Performance**: 9/10 tools correctly categorized (90% accuracy)
- **Exceptional Performance**: 10/10 tools correctly categorized (100% accuracy)

## Tool Categories

### 1. CLI-Based Agents ðŸ–¥ï¸
**Definition**: Terminal-native AI assistants that integrate with command-line workflows

**Key Characteristics**:
- Primary interface is command-line/terminal
- Git workflow integration
- Shell command generation and automation
- Scriptable and batch processing capabilities

### 2. Standalone IDEs ðŸš€
**Definition**: Complete development environments with built-in AI capabilities

**Key Characteristics**:
- Full IDE functionality with native AI integration
- Optimized user interface for AI interaction
- Custom AI model configurations
- Integrated debugging and testing tools

### 3. IDE Extensions ðŸ”Œ
**Definition**: AI plugins that enhance existing development environments

**Key Characteristics** 
- Add AI capabilities to existing IDEs (VS Code, JetBrains, etc.)
- Preserve existing customizations and workflows
- Multiple AI provider support
- Gradual adoption possible

### 4. Online Tools ðŸŒ
**Definition**: Web-based AI coding platforms and services

**Key Characteristics**:
- Browser-based interface
- No local installation required
- Cloud-based AI processing
- Instant collaboration and sharing

## Assessment Tools List

### Exercise Pool (Sample)
*Trainers should select 10 tools from this pool for each assessment*

#### CLI-Based Agents
1. **GitHub Copilot CLI** - Natural language to terminal commands
2. **Aider** - Direct file editing via terminal with Git integration
3. **Warp AI** - AI-powered terminal with built-in assistance
4. **Gemini CLI** - Google's command-line AI assistant
5. **OpenCode CLI** - Open-source terminal AI helper

#### Standalone IDEs
6. **Cursor** - VS Code fork with native AI integration
7. **Windsurf** - Multi-language AI development platform
8. **Zed** - Performance-focused IDE with AI enhancements
9. **Kiro** - AI-first development environment

#### IDE Extensions
10. **GitHub Copilot** - Market-leading VS Code extension
11. **Continue** - Open-source AI coding assistant
12. **Cline** (formerly Claude Dev) - Autonomous development agent
13. **Amazon Q Developer** - AWS-integrated AI assistant
14. **Roo Code** - Multi-model AI extension

#### Online Tools
15. **Bolt.new** - Full-stack app development platform
16. **v0 by Vercel** - UI component generation tool
17. **Replit AI** - Collaborative coding with AI features
18. **Firebase Studio** - Google's AI-powered development environment

## Scoring Rubric

### Categorization Accuracy (Primary Metric)
- **10/10 correct** (100%): Exceptional understanding
- **9/10 correct** (90%): Proficient understanding - **MEETS SC-001**
- **8/10 correct** (80%): Developing understanding - **MEETS SC-001** 
- **7/10 correct** (70%): Beginning understanding - **BELOW THRESHOLD**
- **â‰¤6/10 correct** (â‰¤60%): Inadequate understanding - **REQUIRES REMEDIATION**

### Time Performance (Secondary Metric)
- **â‰¤3 minutes**: Exceptional speed with accuracy
- **3-4 minutes**: Proficient speed 
- **4-5 minutes**: Acceptable speed - **MEETS TIME REQUIREMENT**
- **>5 minutes**: Exceeds time limit - **FAILS TIME REQUIREMENT**

## Assessment Format

### Part A: Tool Categorization (Required)
**Instructions**: "For each tool below, select the correct category: CLI Agent, Standalone IDE, IDE Extension, or Online Tool."

**Example Question**:
> **Tool**: Cursor  
> **Options**: A) CLI Agent, B) Standalone IDE, C) IDE Extension, D) Online Tool  
> **Answer**: B) Standalone IDE

### Part B: Justification (Optional Bonus)
**Instructions**: "Briefly explain why you chose this category for 3 selected tools."

**Scoring**: +0.5 points per correct justification (bonus points)

## Validation Methods

### During Assessment
1. **Timing Enforcement**: Strict 5-minute limit with visual countdown
2. **No External Resources**: Closed-book assessment using only landscape knowledge
3. **Single Attempt**: One opportunity to demonstrate mastery

### Post-Assessment
1. **Immediate Feedback**: Show correct answers and explanations
2. **Gap Analysis**: Identify specific category confusion patterns
3. **Remediation Plan**: Provide targeted review for <80% performers

## Success Indicators

### Individual Level
- **Meets SC-001**: 8+ tools correctly categorized within 5 minutes
- **Demonstrates Understanding**: Can justify category choices with tool characteristics
- **Shows Confidence**: Completes assessment without excessive hesitation

### Cohort Level  
- **Target**: 90% of learners achieve SC-001 threshold
- **Minimum Acceptable**: 80% of learners achieve SC-001 threshold
- **Remediation Trigger**: <75% of learners achieve SC-001 threshold

## Common Misconceptions

### CLI vs Extension Confusion
- **Misconception**: "GitHub Copilot CLI is an extension"
- **Clarification**: CLI version is terminal-native, extension version is IDE-based

### Standalone vs Extension Confusion
- **Misconception**: "Cursor is just a VS Code extension"
- **Clarification**: Cursor is a complete IDE (fork), not an extension

### Online vs Local Tool Confusion
- **Misconception**: "All web-based tools are online tools"
- **Clarification**: Some tools have web interfaces but run locally

## Remediation Strategies

### For <80% Performance
1. **Category Review**: Re-examine tool categories with concrete examples
2. **Hands-on Exploration**: Brief demo of each category type
3. **Decision Framework**: Practice using evaluation criteria
4. **Reassessment**: Second attempt after targeted review

### For Time Limit Failures
1. **Quick Recognition Training**: Flash card-style category identification
2. **Pattern Recognition**: Focus on key distinguishing characteristics
3. **Decision Speed Practice**: Timed mini-assessments with immediate feedback

## Integration with Module Flow

### Pre-Assessment
- **Timing**: After tools landscape presentation, before GitHub Copilot deep dive
- **Purpose**: Validate foundational knowledge before hands-on practice

### Post-Assessment  
- **Success Path**: Proceed to US2 (GitHub Copilot Deep Dive)
- **Remediation Path**: Additional review before advancing
- **Documentation**: Record results for overall module assessment

## Success Validation

### Immediate Validation
Tool categorization accuracy demonstrates landscape understanding necessary for informed tool selection in professional development contexts.

### Long-term Validation
Learners who meet SC-001 show higher success rates in subsequent hands-on tool usage exercises and make more appropriate tool choices in real projects.