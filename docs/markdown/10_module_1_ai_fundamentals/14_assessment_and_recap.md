<!-- .slide: class="transition" -->
# **Module 1: Assessment and Recap**
## **Knowledge Check and Recap**

Notes:
We've covered a lot of ground in Module 1. Let's systematically review the key concepts and test your understanding through practical exercises.

##--##

<!-- .slide: -->
# **Learning Objectives Review**

## **‚úÖ What You Should Now Know**
- üß† **AI Fundamentals**: LLMs, embeddings, and context windows
- üéØ **Prompt Engineering**: CLEAR framework and effective techniques
- üõ†Ô∏è **Practical Application**: Real prompt writing and iteration skills
- üîß **Tool Awareness**: AI coding assistants and their capabilities

## **üí™ Skills You've Developed**
- Writing clear, specific prompts that get better results
- Iteratively refining AI interactions for optimal outcomes
- Applying prompt patterns for different scenarios
- Understanding AI tool capabilities and limitations
- Following safety and best practices for AI usage

Notes:
Take a moment to reflect on your progress. These foundational skills will be the basis for everything we build in the remaining modules.

##--##

<!-- .slide: -->
# **Knowledge Check**

## **Question 1: Context Windows**
<br>

### What is a context window in LLMs and why is it important?

**A)** The visual interface where you interact with AI  
**B)** The maximum amount of text an LLM can process at once  
**C)** The time window when AI models are most accurate  
**D)** The browser window where AI tools run

<details>
<summary>Answer</summary>
**B** - The context window determines how much text (measured in tokens) an LLM can process at once. This affects conversation length, document analysis capability, and code completion quality. Understanding context windows helps you work within AI tool limitations and explains why long conversations may lose coherence.
</details>

Notes:
Context windows are a fundamental concept that directly impacts how you use AI tools in practice.

##--##

<!-- .slide: -->
# **Knowledge Check**

## **Question 2: LLM Performance**
<br>

### Which component is most crucial for LLM performance?

**A)** Processing speed  
**B)** Training data quality and quantity  
**C)** Hardware specifications  
**D)** User interface design

<details>
<summary>Answer</summary>
**B** - While all factors matter, the foundation of any LLM's capabilities comes from the quality, diversity, and quantity of its training data. This determines what the model can understand and generate, and explains why some models excel at certain programming languages while struggling with others.
</details>

Notes:
Training data quality is why different AI models have varying strengths and weaknesses.

##--##

<!-- .slide: -->
# **Resources**

## **Further Learning**
<br>

### **Documentation**
- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/prompt-engineering) - Advanced prompting techniques
- [Google AI Principles](https://ai.google/responsibility/principles/) - AI ethics and responsible use

### **Lab Solutions**
- Review `labs/lab-11-ai-fundamentals-solution/`
- Review `labs/lab-12-prompt-engineering-solution/`

Notes:
Refer back to lab solutions and documentation as references for your continued AI learning journey.