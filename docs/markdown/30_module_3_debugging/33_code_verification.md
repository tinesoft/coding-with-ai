<!-- .slide: class="transition" -->
# Module 3: Verifying AI-Generated Code
## Critical Evaluation and Validation Techniques

##--##

<!-- .slide -->
# Why Verify AI-Generated Code?

## **Understanding AI Limitations**
<br>

AI assistants are powerful but **not infallible**:

- âŒ **Hallucinations**: Nonexistent APIs, packages, or functions
- âš ï¸ **Logic Errors**: Edge cases, off-by-one, incorrect algorithms
- ğŸ”“ **Security Gaps**: SQL injection, XSS, insecure configurations
- ğŸ“‰ **Performance Issues**: N+1 queries, inefficient algorithms
- ğŸ§ª **Testability**: Hard-to-test code, tight coupling
- ğŸ“š **Best Practices**: Outdated patterns, poor naming conventions
- âœ… **Correctness**: Wrong output for specific inputs

Notes:
AI coding assistants dramatically accelerate development, but they're trained on vast datasets that include both good and bad code. They can confidently generate incorrect solutions, invent APIs that don't exist, or overlook critical security concerns. This makes verification a crucial skillâ€”not just for AI-generated code, but for enhancing your overall code review capabilities. The verification mindset helps you catch issues early, whether they come from AI, colleagues, or your own keyboard.

##--##

<!-- .slide -->
# The 7-Point Verification Checklist

## **Systematic Review Framework**
<br>

| # | Check | Question |
|---|-------|----------|
| 1ï¸âƒ£ | **Correctness** | Does it produce expected output for all inputs? |
| 2ï¸âƒ£ | **Logic** | Are edge cases handled? Any off-by-one errors? |
| 3ï¸âƒ£ | **Security** | Are inputs validated? SQL injection risks? |
| 4ï¸âƒ£ | **Dependencies** | Do all imports/packages actually exist? |
| 5ï¸âƒ£ | **Best Practices** | Naming, structure, patterns aligned with standards? |
| 6ï¸âƒ£ | **Performance** | Any obvious inefficiencies (N+1, nested loops)? |
| 7ï¸âƒ£ | **Testability** | Can this code be easily unit tested? |

Notes:
This seven-point checklist provides a systematic approach to code review. Start with correctnessâ€”does it even work? Then examine logic for boundary conditions and edge cases. Security checks should include input validation, authentication, and common vulnerability patterns. Dependencies are particularly important for AI-generated code, which may hallucinate package names or API methods. Best practices ensure maintainability. Performance considerations prevent production bottlenecks. Finally, testability determines long-term code health. The next slide provides a comprehensive visual representation of this framework.

##--##

<img src="./assets/images/module-3/verification-checklist.svg" alt="AI Code Verification Checklist" style="width:100%; height:auto; display:block;">

##--##

<!-- .slide -->
# Verification Workflow

## **Step-by-Step Process**
<br>

```text
1. READ: Understand what the code claims to do
   â†“
2. TEST: Run it with typical inputs
   â†“
3. EDGE CASES: Test boundaries, nulls, empty arrays
   â†“
4. INSPECT: Check dependencies, imports, API calls
   â†“
5. REVIEW: Apply 7-point checklist systematically
   â†“
6. REFINE: Fix issues, ask AI to improve
   â†“
7. VALIDATE: Re-test after modifications
```

Notes:
Effective verification is methodical, not random. Start by reading the code to understand its intentâ€”what problem is it solving? Then run it with normal inputs to establish baseline functionality. Next, push the boundaries with edge cases: empty inputs, null values, maximum sizes, special characters. Inspect external dependencies carefullyâ€”do the packages exist? Are the API signatures correct? Apply the seven-point checklist systematically. When you find issues, collaborate with the AI to fix themâ€”describe the problem and ask for corrections. Finally, validate that your fixes didn't introduce new problems. This workflow becomes faster with practice.

##--##

<!-- .slide -->
# Common AI Code Issues

## **Frequent Problem Patterns**
<br>

### ğŸ”´ Hallucinated Dependencies
```python
from faker_package import generate_data  # Package doesn't exist!
```

### ğŸ”´ Incorrect API Usage
```javascript
array.sortBy('name');  // No such method in JavaScript
```

### ğŸ”´ Missing Edge Cases
```python
def divide(a, b):
    return a / b  # No zero-division check!
```

Notes:
These are the most frequent issues in AI-generated code. Hallucinated dependencies occur when the AI invents plausible-sounding package names or combines real package names with nonexistent APIs. Always verify imports against official documentation. Incorrect API usage happens when the AI conflates methods from different languages or librariesâ€”like using Ruby's sortBy in JavaScript. Missing edge case handling is particularly dangerous because the code works for happy-path scenarios but fails in production. Train yourself to ask: "What could go wrong?"

##--##

<!-- .slide -->
# Testing AI-Generated Code

## **Comprehensive Testing Approach**
<br>

### âœ… **DO**: Test with diverse inputs
```python
test_cases = [
    (normal_input, expected_output),
    ([], expected_for_empty),
    (None, expected_for_null),
    (huge_input, expected_for_large),
]
```

### âŒ **DON'T**: Trust "it looks right"
```python
# AI generated thisâ€”looks plausible, but is it correct?
result = ai_function(data)
# return result  # NO! Test first!
```

Notes:
Testing is your primary defense against AI errors. Create a diverse test suite before integrating generated code: normal cases, edge cases, error conditions, and performance scenarios. Don't let plausible-looking code bypass your testing disciplineâ€”AI is excellent at generating code that looks professional but contains subtle bugs. Use test-driven approaches: describe the expected behavior first, then evaluate the AI's implementation against those expectations. This inverts the trust model: instead of trusting the code and hoping it works, you define success criteria and verify compliance.

##--##

<!-- .slide -->
# Verifying Dependencies

## **Package Validation Process**
<br>

### ğŸ›¡ï¸ **Verification Steps**:

1. Check package exists: `npm search <package>` or search PyPI
2. Verify API documentation on official sites
3. Check version compatibility
4. Test imports before full integration
5. Look for deprecation warnings

### ğŸ’¡ **Pro Tip**: If AI suggests unfamiliar packages, search documentation first!

Notes:
Dependency verification prevents integration disasters. When AI suggests a package, don't immediately install itâ€”search the official registry first (npm, PyPI, Maven Central). Verify that the suggested API methods exist in the official documentation, not just Stack Overflow snippets. Check version compatibility with your project's environment. Test imports in isolation before weaving them into your codebase. Watch for deprecation warningsâ€”AI training data may include outdated patterns. If you're unfamiliar with a package, spend five minutes reviewing its documentation. This small investment prevents hours of debugging mysterious import errors.

##--##

<!-- .slide -->
# Security Verification

## **Identifying Vulnerabilities**
<br>

### ğŸ”“ Common Security Gaps in AI Code:

- **SQL Injection**: Direct string concatenation in queries
- **XSS**: Unescaped user input in HTML
- **Path Traversal**: Unsanitized file paths
- **Hardcoded Secrets**: API keys, passwords in code
- **Missing Authentication**: Unprotected endpoints
- **Insufficient Input Validation**: Trusting user data

### ğŸ›¡ï¸ **Always**: Validate, sanitize, and escape user inputs!

Notes:
AI models are trained on code written before modern security awareness became standard. They may generate code with classic vulnerabilities: SQL queries built with string concatenation, user input rendered directly to HTML without escaping, file paths constructed from user input without validation. Always scrutinize any code that handles user input, database queries, file operations, or network requests. Use parameterized queries for databases, template engines with auto-escaping for HTML, and allowlists for file paths. Never hardcode secretsâ€”use environment variables or secret management systems. Treat all AI-generated code as untrusted input until security review confirms otherwise.

##--##

<!-- .slide -->
# Performance Verification

## **Avoiding Scalability Issues**
<br>

### âš¡ Watch for Performance Anti-Patterns:

```python
# âŒ N+1 Query Problem (AI may not optimize)
for user in users:
    orders = db.query(f"SELECT * FROM orders WHERE user_id = {user.id}")
    
# âœ… Batch Query
user_ids = [user.id for user in users]
orders = db.query(f"SELECT * FROM orders WHERE user_id IN ({user_ids})")
```

```javascript
// âŒ Nested Loop (O(nÂ²))
for (let user of users) {
    for (let order of orders) {
        if (order.userId === user.id) { ... }
    }
}

// âœ… Hash Map Lookup (O(n))
const ordersByUser = new Map();
orders.forEach(o => ordersByUser.set(o.userId, o));
users.forEach(u => { const order = ordersByUser.get(u.id); });
```

Notes:
AI often optimizes for readability over performance, generating code that works correctly but scales poorly. The N+1 query problem is particularly commonâ€”the AI generates a loop that makes one database query per iteration, when a single batched query would suffice. Similarly, nested loops over collections may seem clear but create quadratic complexity. Before accepting generated code, consider its performance characteristics: How will it behave with 1000 items? 100,000 items? Use profiling tools for critical paths. Hash maps, batch operations, and caching can transform algorithmic complexity. Don't optimize prematurely, but don't accept naive implementations in performance-critical code either.

##--##

<!-- .slide -->
# Verifying Code Testability

## **Ensuring Maintainability**
<br>

### âŒ **Hard to Test** (tight coupling):
```javascript
function processOrder(orderId) {
    const db = new Database();  // Hard-coded dependency!
    const order = db.getOrder(orderId);
    const email = new EmailService();
    email.send(order.user.email, "Order confirmed");
}
```

### âœ… **Testable** (dependency injection):
```javascript
function processOrder(orderId, db, emailService) {
    const order = db.getOrder(orderId);
    emailService.send(order.user.email, "Order confirmed");
}
// Easy to mock db and emailService in tests!
```

Notes:
AI-generated code often lacks testability because it creates hard-coded dependenciesâ€”instantiating databases, email services, or external APIs directly inside functions. This makes unit testing nearly impossible without hitting real infrastructure. Look for tight coupling: functions that directly instantiate their dependencies can't be tested in isolation. Refactor for dependency injection, where dependencies are passed as parameters. This enables mocking in tests. AI can help with this refactoringâ€”paste the generated code and ask, "How can I make this more testable using dependency injection?" Testable code is maintainable code, and catching testability issues early prevents technical debt.

##--##

<!-- .slide -->
# Using AI to Verify AI Code

## **Meta-Review Strategy**
<br>

### ğŸ¤– **Meta-Strategy**: Use AI to review AI

**Prompt Pattern**:
```
Review this code for:
1. Correctness issues
2. Security vulnerabilities  
3. Performance bottlenecks
4. Missing edge cases
5. Dependency hallucinations

[paste code]

What issues do you find?
```

### âš ï¸ **Remember**: AI can find issues it createdâ€”but **you** make final decisions!

Notes:
Here's a powerful technique: use AI to review AI-generated code. Paste generated code back into the chat with a systematic review prompt covering the seven-point checklist. AI can often spot its own mistakes when prompted to look criticallyâ€”particularly hallucinations, incorrect API usage, and missing edge cases. However, don't trust this meta-review blindly. AI models can contradict themselves or miss issues they highlighted moments ago. Use AI-powered review as one tool in your verification toolkit, complementing manual inspection, testing, and documentation checks. The human developer remains the final arbiter of code quality. Think of AI review as a second pair of eyes, not a replacement for your judgment.

##--##

<!-- .slide -->
# Best Practices for Code Verification

## **Professional Standards**
<br>

1. âœ… **Always test** before integrating AI code
2. ğŸ“š **Check documentation** for unfamiliar APIs/packages
3. ğŸ§ª **Write tests first** (TDD approach)
4. ğŸ” **Manual review** critical sections (auth, payment, security)
5. ğŸ¤– **Use AI for review** but verify AI's verification
6. ğŸ“Š **Track patterns** in AI errors to improve prompts
7. ğŸ”„ **Iterate**: Fix, re-test, refine

Notes:
Synthesizing verification into best practices: test everything, check official documentation for any unfamiliar code, consider test-driven development where you write tests before accepting AI solutions, manually review critical sections like authentication and payment processing, use AI to review AI but verify those findings independently, track common error patterns to improve your prompting strategy, and iterate until the code meets your quality bar. Verification becomes faster with practiceâ€”you'll develop intuition for red flags like unusual package names, overly complex logic, or missing error handling. Make verification a habit, not an afterthought.

##--##

<!-- .slide -->
# Summary: Code Verification Mindset

## **Key Principles**
<br>

| Principle | Why It Matters |
|-----------|----------------|
| ğŸ¯ **Trust but Verify** | AI is a tool, not a source of truth |
| ğŸ§ª **Test Everything** | Bugs found early are easier to fix |
| ğŸ“š **Documentation First** | Official docs > AI suggestions |
| ğŸ”’ **Security Always** | One vulnerability can compromise everything |
| âš¡ **Performance Matters** | Slow code becomes slower at scale |
| ğŸ§ª **Testability Counts** | Untestable code becomes unmaintainable |
| ğŸ”„ **Iterate and Improve** | First draft is rarely the best solution |

Notes:
Adopt a verification mindset that balances speed with quality. Trust AI to accelerate your work, but verify every significant suggestion through testing, documentation review, and critical thinking. Test everythingâ€”the cost of finding bugs in development is trivial compared to production incidents. Always consult official documentation before trusting AI's API usage. Security cannot be an afterthoughtâ€”one SQL injection vulnerability can compromise an entire system. Consider performance implications early, especially for code that will run at scale. Prioritize testability to ensure long-term maintainability. Finally, iterateâ€”AI's first suggestion is a starting point, not a final solution. With practice, verification becomes second nature, making you a more effective developer regardless of whether you're using AI assistance.

##--##

<!-- .slide: class="exercice" -->
# Exercise 3: Code Verification
## Lab 33

<br>

### ğŸ‘‰ğŸ¾ Follow **instructions** in

### `labs/lab-33-code-verification/README.md`

Notes:
- Follow the instructions in the lab README.md for detailed steps.

##--##

<!-- .slide: class="exercice" -->
# Exercise 3: Code Verification
## Lab 33

<br>

### ğŸ’¡ Read **solution** at

### `labs/lab-33-code-verification-solution/README.md`

Notes:
- Read the lab README.md for example of solutions.
